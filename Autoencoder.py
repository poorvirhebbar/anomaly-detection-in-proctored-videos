# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19PLR-Bj7bBJ9G9gj6KP-Qqswc9uv4TCj
"""

import numpy as np 
import matplotlib.pyplot as plt 
from random import randint 
from keras import backend as K 
from keras.layers import Input, Dense, Conv2D, MaxPooling2D, UpSampling2D 
from keras.models import Model 
from keras.datasets import mnist 
from keras.callbacks import TensorBoard 
from google.colab import drive
drive.mount('/content/drive')

from sklearn.preprocessing import normalize
def load_data(): 
    # defining the input image size  
    input_image = Input(shape =(20,52, 1)) 
      
    # Loading the data and dividing the data into training and testing sets 
    (X_train, _), (X_test, _) = mnist.load_data() 
    # print(type(X_train))
    f1=open('/content/drive/My Drive/Colab Notebooks/res_tt.txt','r')
    f2=open('/content/drive/My Drive/Colab Notebooks/res_ff.txt','r')
    X_train1=[]
    X_test1=[]
    X_train2=[]
    X_test2=[]
    fl1=[]
    for l1 in f1:
      fl1.append(l1)

    fl2=[]
    for l2 in f2:
      fl2.append(l2)
      

    
    for i in range(121):
      if i>100:
        al=[]
        for j in range(20):
          for k in range(52):
            al.append(fl1[i*52+j*52+k])
        X_test1.append(al)

      else:
        al=[]
        for j in range(20):
          for k in range(52):
            al.append(fl1[i*52+j*52+k])
        X_train1.append(al)
      

    for i in range(121):
      if i>100:
        al=[]
        for j in range(20):
          for k in range(52):
            al.append(fl2[i*52+j*52+k])
        X_test2.append(al)
      else:
        al=[]
        for j in range(20):
          for k in range(52):
            al.append(fl2[i*52+j*52+k])
        X_train2.append(al)

    
    X_train1 = np.array( X_train1)
    X_train2 = np.array(X_train2)



    
    
      
    # Cleaning and reshaping the data as required by the model 
    X_train1 = X_train1.astype('float32') 
    X_train1 = np.reshape(X_train1, (len(X_train1), 20,52, 1)) 
    X_train2 = X_train2.astype('float32') 
    X_train2 = np.reshape(X_train2, (len(X_train2), 20,52, 1)) 
    X_test1 = np.array(X_test1)
    X_test2 = np.array(X_test2)



    
    
      
    # Cleaning and reshaping the data as required by the model 
    X_test1 = X_test1.astype('float32') 
    X_test1 = np.reshape(X_test1, (len(X_test1), 20,52, 1)) 
    X_test2 = X_test2.astype('float32') 
    X_test2 = np.reshape(X_test2, (len(X_test2), 20,52, 1)) 
    # X_test = X_test.astype('float32') 
    # X_test = np.reshape(X_test, (len(X_test), 20, 52, 1)) 
      
    return X_train1, X_test1,X_train2, X_test2, input_image 

X_train1, X_test1,X_train2, X_test2, input_image = load_data()
# print(X_train1)
# X_train2=normalize(X_train2, axis=1)
# print(X_train2)
v_min = X_train2.min(axis=(0, 1), keepdims=True)
v_max = X_train2.max(axis=(0, 1), keepdims=True)
X_train2=(X_train2 - v_min)/(v_max - v_min)
print(X_train2)
v_min = X_train1.min(axis=(0, 1), keepdims=True)
v_max = X_train1.max(axis=(0, 1), keepdims=True)
X_train1=(X_train1 - v_min)/(v_max - v_min)

v_min = X_test2.min(axis=(0, 1), keepdims=True)
v_max = X_test2.max(axis=(0, 1), keepdims=True)
X_test2=(X_test2 - v_min)/(v_max - v_min)
print(X_test2)
v_min = X_test1.min(axis=(0, 1), keepdims=True)
v_max = X_test1.max(axis=(0, 1), keepdims=True)
X_test1=(X_test1 - v_min)/(v_max - v_min)

from keras.models import Model
from keras.layers.convolutional import Conv2D, Conv2DTranspose
from keras.layers.convolutional_recurrent import ConvLSTM2D
from keras.layers.normalization import BatchNormalization
from keras.layers.wrappers import TimeDistributed
from keras.layers.core import Activation
from keras.layers import Input

def build_network(input_image): 
		# conv1_1 = Conv2D(32, (3, 3), activation='relu', padding='same')(x)
		# pool1 = MaxPooling2D((2, 2), padding='same')(conv1_1)
		# conv1_2 = Conv2D(32, (3, 3), activation='relu', padding='same')(pool1)
		# h = MaxPooling2D((2, 2), padding='same')(conv1_2)


		# # Decoder
		# conv2_1 = Conv2D(32, (3, 3), activation='relu', padding='same')(h)
		# up1 = UpSampling2D((2, 2))(conv2_1)
		# conv2_2 = Conv2D(32, (3, 3), activation='relu', padding='same')(up1)
		# up2 = UpSampling2D((2, 2))(conv2_2)
		# r = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(up2)
      
    # bb=Autoencoder()
    # return bb.build(input_image)
    # Building the encoder of the Auto-encoder 
    x = Conv2D(128, (3, 3), activation ='relu', padding ='same')(input_image) 
    x = MaxPooling2D((2, 2), padding ='same')(x) 
    # x = (BatchNormalization())(x)
    x = Conv2D(64, (3, 3), activation ='relu', padding ='same')(x) 
    # x = BatchNormalization()(x)
    x = MaxPooling2D((2, 2), padding ='same')(x) 
    x = Conv2D(8, (3, 3), activation ='relu', padding ='same')(x) 
    encoded_layer = MaxPooling2D((2, 2), padding ='same')(x) 
      
    # Building the decoder of the Auto-encoder 
    x = Conv2D(8, (3, 3), activation ='relu', padding ='same')(encoded_layer) 
    x = UpSampling2D((2, 2))(x) 
    x = Conv2D(8, (3, 3), activation ='relu', padding ='same')(x) 
    x = UpSampling2D((2, 2))(x) 
    x = Conv2D(16, (3, 3), activation ='relu')(x) 
    x = UpSampling2D((2, 2))(x) 
    decoded_layer = Conv2D(1, (3, 3), activation ='sigmoid', padding ='same')(x) 
      
    return decoded_layer 
# CLASS FOR THE MODEL. OBJECT WILL BE INSTANTIATED AND USED FOR THE PROCESSING
class Autoencoder:
	def __init__(self):
		print("Model object created")

	# MODULE THAT CONTAINS THE ARCHITECTURE OF THE MODEL AND PASSES THE INPUT THROUGH THE MODEL
	def build(self,inp):

		# ENCODER
		
		# 1ST CONVOLUTION LAYER
		self.conv1 = self.conv_layer(inp,11,4,128,'conv1')
		self.conv1 = self.batch_norm(self.conv1)
		self.conv1 = self.relu_oper(self.conv1)

		
		# 2ND CONVOLUTION LAYER
		self.conv2 = self.conv_layer(self.conv1,5,2,64,'conv2')
		self.conv2 = self.batch_norm(self.conv2)
		self.conv2 = self.relu_oper(self.conv2)

		
		# CONVOLUTION LSTM
		self.conv_lstm_1 = self.conv_lstm(self.conv2,64,3,'convlstm1')

		self.conv_lstm_2 = self.conv_lstm(self.conv_lstm_1,32,3,'convlstm2')

		self.conv_lstm_3 = self.conv_lstm(self.conv_lstm_2,64,3,'convlstm3')

		# 1ST DECONVOLUTION LAYER
		self.deconv1 = self.deconv_layer(self.conv_lstm_3,5,2,128,'deconv1')
		self.deconv1 = self.batch_norm(self.deconv1)
		self.deconv1 = self.relu_oper(self.deconv1)

		# RECONSTRUCTION OF THE ORIGINAL IMAGE
		self.decoded = self.deconv_layer(self.deconv1,11,4,1,'deconv2')
		

		return (self.decoded)
  
  

		

	# MODULE THAT CREATES A TIME_DISTRIBUTED 2D-CONVOLUTION LAYER
	def conv_layer(self,bottom,kernel_dim,stride,out_channel,name):
	       
        # DEFINING A CONVOLUTION FILTER
		conv = TimeDistributed(Conv2D(out_channel, kernel_size=(kernel_dim, kernel_dim), padding='same', strides=(stride, stride), name=name))(bottom)
		
		return conv


	# MODULE THAT CREATES A TIME_DISTRIBUTED 2D-DECONVOLUTION LAYER
	def deconv_layer(self,bottom,kernel_dim,stride,out_channel,name):
		
		deconv = TimeDistributed(Conv2DTranspose(out_channel, kernel_size=(kernel_dim, kernel_dim), padding='same', strides=(stride, stride), name=name))(bottom)

		return deconv




	# SOMETHING CALLED 'return_sequences = True'. ALSO, PADDING ISN'T PRESENT.
	def conv_lstm(self,bottom,out_channel,kernel_dim,name):
		
		lstm = ConvLSTM2D(out_channel, kernel_size=(kernel_dim, kernel_dim), padding='same', return_sequences=True, name=name)(bottom)

		return lstm

	
	# MODULE THAT DOES A TIME_DISTRIBUTED BATCH_NORMALIZATION ON A GIVEN TENSOR 
	def batch_norm(self,bottom):
		norm = TimeDistributed(BatchNormalization())(bottom)	
		return norm


	# MODULE THAT EVALUATES A TIME_DISTRIBUTED RELU OPERATION 
	def relu_oper(self,bottom):
		relu = TimeDistributed(Activation('relu'))(bottom)
		return relu

# def custom_loss(layer):

#     # Create a loss function that adds the MSE loss to the mean of all squared activations of a specific layer
#     def loss(y_true,y_pred):
#         return -K.mean(K.square(y_pred - y_true))
   
#     # Return a function
#     return loss
import keras.backend as K
import numpy as np
r1=0.1
r2=1
X_train= np.concatenate((X_train1,X_train2),axis=0)
rating=np.concatenate((np.full(X_train1.shape,r1),np.full(X_train2.shape,r2)),axis=0)

shuffler = np.random.permutation(len(X_train))
array1_shuffled = X_train[shuffler]
array2_shuffled = rating[shuffler]
weight=dict()
i=0
# for el in array1_shuffled:
#   print(array2_shuffled[i])
#   weight[el]=array2_shuffled[i]
#   i+=1
# print(array1_shuffled)
def customLoss(yTrue,yPred):
    # print(yTrue)
    return 0.1*K.mean(K.square(yPred - yTrue))
def build_auto_encoder_model( input_image, decoded_layer): 
      
    # Defining the parameters of the Auto-encoder 
    autoencoder = Model(input_image, decoded_layer) 
    autoencoder.compile(optimizer ='adam', loss ='mse') 
      
    # Training the Auto-encoder 
    autoencoder.fit(X_train1, X_train1, 
                epochs = 150, 
                batch_size = 10, 
                shuffle = True
                ) 
    
    autoencoder.compile(optimizer ='adam', loss ='mse') 
    autoencoder.fit(X_train2, X_train2, 
                epochs = 30, 
                batch_size = 10, 
                shuffle=True
               )
    autoencoder.compile(optimizer ='adam', loss ='mse') 
      
    # Training the Auto-encoder 
    autoencoder.fit(X_train1, X_train1, 
                epochs = 150, 
                batch_size = 10, 
                shuffle = True
                ) 
    
    #             callbacks =[TensorBoard(log_dir ='/tmp / autoencoder')]) 
      
    return autoencoder

# X_train, X_test, input_image = load_data() 
decoded_layer = build_network(input_image) 

auto_encoder_model = build_auto_encoder_model(
                                             input_image, 
                                             decoded_layer)

reconstructed_images = auto_encoder_model.predict(X_test2) 
reconstructed_images2 = auto_encoder_model.predict(X_test1)

from sklearn.metrics import mean_squared_error 
fs=0
fs2=0
for el in range(len(reconstructed_images)):
  s=0
  s2=0
  for e in range(len(reconstructed_images[el])):
    # pr int((mean_squared_error(reconstructed_images[el][e],X_train2[el][e])))
    s+=(mean_squared_error(reconstructed_images[el][e],X_test2[el][e]))
    s2+=(mean_squared_error(reconstructed_images2[el][e],X_test1[el][e]))
  print(s)
  print(s2)
  fs+=s
  fs2+=s2
print(fs)
print(fs2)